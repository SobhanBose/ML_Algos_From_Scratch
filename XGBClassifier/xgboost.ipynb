{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import utils\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv(\"../dataset/Student Stress Factors (2).csv\")\n",
    "# dataset.columns = [\"Sleep Quality\", \"Headache Frequency\", \"Academic Performance\", \"Study Load\", \"Extracurricular Frequency\", \"Stress Level\"]\n",
    "# dataset.iloc[:,-1] = dataset.iloc[:,-1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset = pd.read_csv(\"../dataset/diabetes_risk_prediction_dataset.csv\")\n",
    "df = dataset.copy()\n",
    "\n",
    "for c in df.columns:\n",
    "    if c != \"Age\":\n",
    "        encoder = LabelEncoder()\n",
    "        dataset[c] = encoder.fit_transform(df[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = utils.train_valid_split(dataset, test_size=0.1, random_state=11)\n",
    "\n",
    "# X_valid = valid_df.copy()\n",
    "# y_valid = X_valid[\"Stress Level\"]\n",
    "# X_valid.drop([\"Stress Level\"], axis=1, inplace=True)\n",
    "\n",
    "# X_train = train_df.copy()\n",
    "# y_train = X_train[\"Stress Level\"]\n",
    "# X_train.drop([\"Stress Level\"], axis=1, inplace=True)\n",
    "\n",
    "X_valid = valid_df.copy()\n",
    "y_valid = X_valid[\"class\"]\n",
    "X_valid.drop([\"class\"], axis=1, inplace=True)\n",
    "\n",
    "X_train = train_df.copy()\n",
    "y_train = X_train[\"class\"]\n",
    "X_train.drop([\"class\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building XGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    def __init__(self, left_child: np.array=None, right_child: np.array=None, split_feature: int=None, split_thresh: float=None, gain: float=-1) -> None:\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.split_feature = split_feature\n",
    "        self.split_thresh = split_thresh\n",
    "        self.gain = gain\n",
    "        self.is_leaf = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "    def __init__(self, output_value: float=None) -> None:\n",
    "        self.output_value = output_value\n",
    "        self.is_leaf = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBTree:\n",
    "    def __init__(self, lamda: float=0, max_depth: int=6, min_child_weight: int=0) -> None:\n",
    "        \"\"\"\n",
    "        XGBTree class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lamda : float, optional\n",
    "            L2 regularization term on weights, by default 0\n",
    "        max_depth : int, optional\n",
    "            Maximum tree depth for base learners, by default 6\n",
    "        min_child_weight : int, optional\n",
    "            Minimum sum of instance weight(hessian) needed in a child., by default 0\n",
    "        \"\"\"\n",
    "        self.lamda = lamda\n",
    "        self.max_depth = max_depth\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.root = None\n",
    "\n",
    "    \n",
    "    def __getResiduals(self, df: np.ndarray, prev_proba) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get residuals from probabilities\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : np.ndarray\n",
    "            Dataset\n",
    "        prev_proba : _type_\n",
    "            Probability calculated by XGB\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array of residuals\n",
    "        \"\"\"\n",
    "        Y = df[:,-1]\n",
    "        residuals = Y-prev_proba\n",
    "        return residuals\n",
    "    \n",
    "\n",
    "    def __getSimilarityScore(self, residuals: np.array, prev_proba: np.array) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Similarity Score of a node\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        residuals : np.array\n",
    "            Array of residuals\n",
    "        prev_proba : np.array\n",
    "            Corresponding probabilities calculated by XGB\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Similarity Score\n",
    "        \"\"\"\n",
    "        return np.sum(residuals)**2/(np.sum(prev_proba * (1-prev_proba)) + self.lamda)\n",
    "\n",
    "\n",
    "    def __getCover(self, prev_proba: np.array) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Cover values of a node\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prev_proba : np.array\n",
    "            Probabilities calculated by XGB\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Cover\n",
    "        \"\"\"\n",
    "        return np.sum(prev_proba * (1-prev_proba))\n",
    "    \n",
    "\n",
    "    def __getOutputValue(self, residuals: np.array, prev_proba: np.array) -> float:\n",
    "        \"\"\"\n",
    "        Calculate output values of a leaf\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        residuals : np.array\n",
    "            Array of residuals\n",
    "        prev_proba : np.array\n",
    "            Corresponding probabilities calculated by XGB\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Output value\n",
    "        \"\"\"\n",
    "        return np.sum(residuals)/(np.sum(prev_proba * (1-prev_proba)) + self.lamda)\n",
    "    \n",
    "    \n",
    "    def __split(self, df: np.ndarray, residuals: np.array, prev_proba: np.array, feature_indx: int, split_thresh: float) -> tuple:\n",
    "        \"\"\"\n",
    "        Split the node based on a feature and threshold\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : np.ndarray\n",
    "            Dataset\n",
    "        residuals : np.array\n",
    "            Array of residuals\n",
    "        prev_proba : np.array\n",
    "            Corresponding probabilities calculated by XGB\n",
    "        feature_indx : int\n",
    "            Index of feature to split on\n",
    "        split_thresh : float\n",
    "            Threshold of feature to split on\n",
    "        \"\"\"\n",
    "        left_residuals = list()\n",
    "        left_dataset = list()\n",
    "        right_residuals = list()\n",
    "        right_dataset = list()\n",
    "        left_prev_proba = list()\n",
    "        right_prev_proba = list()\n",
    "\n",
    "        for row_indx in range(df.shape[0]):\n",
    "            if df[row_indx, feature_indx] <= split_thresh:\n",
    "                left_residuals.append(residuals[row_indx])\n",
    "                left_dataset.append(df[row_indx])\n",
    "                left_prev_proba.append(prev_proba[row_indx])\n",
    "            else:\n",
    "                right_residuals.append(residuals[row_indx])\n",
    "                right_dataset.append(df[row_indx])\n",
    "                right_prev_proba.append(prev_proba[row_indx])\n",
    "\n",
    "        return np.array(left_residuals), np.array(right_residuals), np.array(left_dataset), np.array(right_dataset), np.array(left_prev_proba), np.array(right_prev_proba)\n",
    "    \n",
    "\n",
    "    def __calculateGain(self, root: np.array, left: np.array, right: np.array, root_prev_proba, \n",
    "                            left_prev_proba: np.array, right_prev_proba: np.array) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Gain for a split\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        root : np.array\n",
    "            Residuals in root node\n",
    "        left : np.array\n",
    "            Residuals in left node\n",
    "        right : np.array\n",
    "            Residuals in right node\n",
    "        root_prev_proba : _type_\n",
    "            prev_proba in root node\n",
    "        left_prev_proba : np.array\n",
    "            Correcponding prev_proba in left node\n",
    "        right_prev_proba : np.array\n",
    "            Correcponding prev_proba in right node\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Gain\n",
    "        \"\"\"\n",
    "        similarity_root = self.__getSimilarityScore(root, root_prev_proba)\n",
    "        similarity_left = self.__getSimilarityScore(left, left_prev_proba)\n",
    "        similarity_right = self.__getSimilarityScore(right, right_prev_proba)\n",
    "\n",
    "        return similarity_left+similarity_right-similarity_root\n",
    "    \n",
    "\n",
    "    def __getBestSplit(self, df: np.ndarray, residuals: np.array, prev_proba: np.array, feature_indices: np.array) -> dict:\n",
    "        \"\"\"\n",
    "        Get the best split\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : np.ndarray\n",
    "            Dataset\n",
    "        residuals : np.array\n",
    "            Residuals\n",
    "        prev_proba : np.array\n",
    "            Probabilities calculated by XGB\n",
    "        feature_indices : np.array\n",
    "            Feature indices to split on\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            best_split\n",
    "        \"\"\"\n",
    "        best_split = {'gain': -1, 'feature': None, 'split_thresh': None}\n",
    "\n",
    "        for feature_indx in feature_indices:\n",
    "            feature_values = df[:,feature_indx]\n",
    "            thresholds = np.unique(feature_values)\n",
    "            thresholds = np.array([(thresholds[i]+thresholds[i+1])/2 for i in range(len(thresholds)-1)])\n",
    "            for threshold in thresholds:\n",
    "                left_res, right_res, left_dataset, right_dataset, left_prev_proba, right_prev_proba = self.__split(df, residuals, prev_proba, feature_indx, threshold)\n",
    "                if len(left_res) and len(right_res):\n",
    "                    gain = self.__calculateGain(residuals, left_res, right_res, prev_proba, left_prev_proba, right_prev_proba)\n",
    "                    if gain > best_split[\"gain\"]:\n",
    "                        best_split[\"feature\"] = feature_indx\n",
    "                        best_split[\"split_thresh\"] = threshold\n",
    "                        best_split[\"left_residuals\"] = left_res\n",
    "                        best_split[\"right_residuals\"] = right_res\n",
    "                        best_split[\"left_dataset\"] = left_dataset\n",
    "                        best_split[\"right_dataset\"] = right_dataset\n",
    "                        best_split[\"left_prev_proba\"] = left_prev_proba\n",
    "                        best_split[\"right_prev_proba\"] = right_prev_proba\n",
    "                        best_split[\"gain\"] = gain\n",
    "        \n",
    "        return best_split\n",
    "    \n",
    "\n",
    "    def __buildTreeRecur(self, df: np.ndarray, residuals: np.array, prev_proba: np.array, depth: int=0) -> DecisionNode|Leaf:\n",
    "        \"\"\"\n",
    "        Build XGBTree recursively\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : np.ndarray\n",
    "            Dataset\n",
    "        residuals : np.array\n",
    "            Residuals\n",
    "        prev_proba : np.array\n",
    "            Probabilities calculated by XGB\n",
    "        depth : int, optional\n",
    "            Current depth, by default 0\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DecisionNode|Leaf\n",
    "        \"\"\"\n",
    "        X, y = df[:,:-1], df[:,-1]\n",
    "\n",
    "        if self.__getCover(prev_proba)>self.min_child_weight and depth<=self.max_depth:\n",
    "            best_split = self.__getBestSplit(df, residuals, prev_proba, np.arange(X.shape[1]))\n",
    "            if best_split[\"gain\"] >= 0:\n",
    "                left_residuals = self.__buildTreeRecur(best_split[\"left_dataset\"], best_split[\"left_residuals\"], best_split[\"left_prev_proba\"], depth+1)\n",
    "                right_residuals = self.__buildTreeRecur(best_split[\"right_dataset\"], best_split[\"right_residuals\"], best_split[\"right_prev_proba\"], depth+1)\n",
    "                \n",
    "                return DecisionNode(left_residuals, right_residuals, best_split[\"feature\"], best_split[\"split_thresh\"], \n",
    "                            best_split[\"gain\"])\n",
    "        \n",
    "        output_value = self.__getOutputValue(residuals, prev_proba)\n",
    "        return Leaf(output_value)\n",
    "    \n",
    "    \n",
    "    def fit(self, df: pd.DataFrame, prev_proba: np.array):\n",
    "        \"\"\"\n",
    "        Fit the XGBTree to the data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Dataset\n",
    "        prev_proba : np.array\n",
    "            Probability of base estimator\n",
    "        \"\"\"\n",
    "        residuals = self.__getResiduals(df.to_numpy(), prev_proba)\n",
    "        self.root = self.__buildTreeRecur(df.to_numpy(), residuals, prev_proba)\n",
    "\n",
    "\n",
    "    def make_prediction(self, X: np.array, node: DecisionNode|Leaf) -> int:\n",
    "        \"\"\"\n",
    "        Make individual prediction\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Data\n",
    "        node : DecisionNode | Leaf\n",
    "            root node\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Output value\n",
    "        \"\"\"\n",
    "        if node.is_leaf: \n",
    "            return node.output_value\n",
    "        else:\n",
    "            feature = X[node.split_feature]\n",
    "            if feature <= node.split_thresh:\n",
    "                return self.make_prediction(X, node.left_child)\n",
    "            else:\n",
    "                return self.make_prediction(X, node.right_child)\n",
    "    \n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.array:\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            Output values\n",
    "        \"\"\"\n",
    "        output_values = [self.make_prediction(x, self.root) for x in X]\n",
    "        return np.array(output_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(action=\"error\", category=RuntimeWarning)\n",
    "# warnings.filterwarnings(action=\"ignore\", category=DeprecationWarning)\n",
    "from math import log\n",
    "\n",
    "class XGBClassifier:\n",
    "    def __init__(self, n_estimators: int=100, max_depth: int=6, eta: float=0.3, lamda: float=0, gamma: float=0, num_class: int=2, \n",
    "                    min_child_weight: int=0) -> None:\n",
    "        \"\"\"\n",
    "        XGBClassifier class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_estimators : int, optional\n",
    "            Number of gradient boosted trees, by default 100\n",
    "        max_depth : int, optional\n",
    "            Maximum tree depth for base learners, by default 6\n",
    "        eta : float, optional\n",
    "            Boosting learning rate, by default 0.3\n",
    "        lamda : float, optional\n",
    "            L2 regularization term on weights, by default 0\n",
    "        num_class : int, optional\n",
    "            Number of class labels, by default 2\n",
    "        min_child_weight : int, optional\n",
    "            Minimum sum of instance weight(hessian) needed in a child, by default 0\n",
    "        \"\"\"\n",
    "        self.num_class = num_class\n",
    "        self.n_estimators = n_estimators\n",
    "        self.eta = eta\n",
    "        self.max_depth = max_depth\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.lamda = lamda\n",
    "        self.estimators = []\n",
    "        for _ in range(self.num_class):\n",
    "            self.estimators.append([XGBTree(max_depth=self.max_depth, lamda=self.lamda) for _ in range(n_estimators)])\n",
    "\n",
    "    \n",
    "    def __getLogOdds(self, probabilities: float|np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Get log(odds)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        probabilities : float | np.ndarray\n",
    "            Probabilities\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            log(odds)\n",
    "        \"\"\"\n",
    "        if isinstance(probabilities, np.ndarray):\n",
    "            odds = []\n",
    "            for prob in probabilities:\n",
    "                odds.append(prob/1-prob)\n",
    "            log_odds = []\n",
    "            for odd in odds:\n",
    "                if odd != 0:\n",
    "                    log_odds.append(log(odd))\n",
    "                else:\n",
    "                    log_odds.append(0)\n",
    "            return np.array(log_odds)\n",
    "        else:\n",
    "            odds = probabilities/1-probabilities\n",
    "            if odds>0:\n",
    "                return np.log(odds)\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def __sigmoid(self, z: float|np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate sigmoid(z)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : float | np.ndarray\n",
    "            Data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            sidmoid(z)\n",
    "        \"\"\"\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Fit the XGBClassifier to data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Dataset\n",
    "        \"\"\"\n",
    "        self.base_probability = np.array([0.5] * df.shape[0])\n",
    "        for label in range(self.num_class):\n",
    "            df_transformed = df.copy()\n",
    "            df_transformed.iloc[:,-1] = np.where(df_transformed.iloc[:,-1]==label, 1, 0)\n",
    "\n",
    "            calc_prob = np.array([self.base_probability] * df.shape[0])\n",
    "            output_values = np.zeros(df.shape[0])\n",
    "            for estimator in self.estimators[label]:\n",
    "                estimator.fit(df_transformed, calc_prob)\n",
    "                output_values += estimator.predict(df_transformed.to_numpy()[:,:-1])\n",
    "                calc_prob = self.__sigmoid(self.__getLogOdds(self.base_probability) + self.eta*output_values)\n",
    "\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input Data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Predictions\n",
    "        \"\"\"\n",
    "        y_hat = []\n",
    "        for x in X.to_numpy():\n",
    "            one_vs_all = {}\n",
    "            for i, eestimators in enumerate(self.estimators):\n",
    "                output_values = [estimator.make_prediction(x, estimator.root) for estimator in eestimators]\n",
    "                one_vs_all[i] = np.mean(self.__sigmoid(self.__getLogOdds(self.base_probability) + (self.eta*sum(output_values))))\n",
    "\n",
    "            y_hat.append(max(one_vs_all, key=one_vs_all.get))\n",
    "\n",
    "        return np.array(y_hat).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(n_estimators=100, num_class=2)\n",
    "xgb_clf.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting predictions of validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = xgb_clf.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy Score: {utils.get_accuracy_score(y_valid, y_hat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.923768884788375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f\"F1 Score: {f1_score(y_valid, y_hat, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
